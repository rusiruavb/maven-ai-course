{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain & LangGraph Assessment\n",
    "\n",
    "This notebook demonstrates:\n",
    "1. LangChain RAG with Conversation Memory\n",
    "2. LangGraph workflow with conditional Summarizer node\n",
    "3. Reflection on LangChain vs LangGraph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, List\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "load_dotenv()\n",
    "print(\"✓ Dependencies loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Data: The Matrix Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"\"\"The Matrix is a 1999 science fiction action film written and directed by the Wachowskis. \n",
    "    The film depicts a dystopian future in which humanity is unknowingly trapped inside the Matrix, \n",
    "    a simulated reality that intelligent machines have created to distract humans while using their \n",
    "    bodies as an energy source.\"\"\",\n",
    "    \n",
    "    \"\"\"The main character is Neo, a computer programmer and hacker whose real name is Thomas Anderson. \n",
    "    Neo is contacted by Morpheus, who reveals the truth about the Matrix. Neo is believed to be 'The One', \n",
    "    a prophesied individual who will end the war between humans and machines.\"\"\",\n",
    "    \n",
    "    \"\"\"Other key characters include Trinity, a skilled hacker and Morpheus's second-in-command who becomes \n",
    "    Neo's love interest, and Agent Smith, a sentient program designed to eliminate threats to the Matrix. \n",
    "    The film explores themes of reality, free will, and the nature of consciousness.\"\"\",\n",
    "    \n",
    "    \"\"\"In the climactic scenes, Neo learns to manipulate the Matrix and gains superhuman abilities. \n",
    "    He can dodge bullets, fly, and even perceive the underlying code of the Matrix. The film ends with \n",
    "    Neo promising to show people a world without boundaries, where anything is possible.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"✓ Loaded {len(documents)} documents about The Matrix\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LangChain RAG with Conversation Memory\n",
    "\n",
    "This implementation adds conversation memory so the system can answer follow-up questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50\n",
    ")\n",
    "\n",
    "splits = text_splitter.create_documents(documents)\n",
    "\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "\n",
    "print(f\"✓ Created vector store with {len(splits)} chunks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n",
    "memory = ConversationBufferMemory(\n",
    "    memory_key=\"chat_history\",\n",
    "    return_messages=True,\n",
    "    output_key=\"answer\"\n",
    ")\n",
    "\n",
    "conversational_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    memory=memory,\n",
    "    return_source_documents=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"✓ Conversational RAG chain created with memory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Conversation Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Q1: Tell me about the plot of The Matrix.\\n\")\n",
    "result1 = conversational_chain({\"question\": \"Tell me about the plot of The Matrix.\"})\n",
    "print(f\"A1: {result1['answer']}\\n\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQ2: Who is the main character?\\n\")\n",
    "result2 = conversational_chain({\"question\": \"Who is the main character?\"})\n",
    "print(f\"A2: {result2['answer']}\\n\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nQ3: What special abilities does he develop?\\n\")\n",
    "result3 = conversational_chain({\"question\": \"What special abilities does he develop?\"})\n",
    "print(f\"A3: {result3['answer']}\\n\")\n",
    "print(\"\\n✓ Memory working correctly - 'he' refers to Neo from previous conversation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LangGraph Workflow with Conditional Summarizer\n",
    "\n",
    "This implementation includes:\n",
    "- A retriever node\n",
    "- A conditional summarizer node (activates if documents are too long)\n",
    "- A final answer generation node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[str]\n",
    "    summarized_docs: str\n",
    "    answer: str\n",
    "    needs_summary: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_documents(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    doc_texts = [doc.page_content for doc in docs]\n",
    "    \n",
    "    total_length = sum(len(doc) for doc in doc_texts)\n",
    "    needs_summary = total_length > 800\n",
    "    \n",
    "    print(f\"Retrieved {len(docs)} documents (total length: {total_length} chars)\")\n",
    "    print(f\"Needs summarization: {needs_summary}\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"documents\": doc_texts,\n",
    "        \"needs_summary\": needs_summary\n",
    "    }\n",
    "\n",
    "def summarize_documents(state: GraphState) -> GraphState:\n",
    "    documents = state[\"documents\"]\n",
    "    combined_docs = \"\\n\\n\".join(documents)\n",
    "    \n",
    "    summarizer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Summarize the following documents concisely while retaining key information:\n",
    "        \n",
    "        {documents}\n",
    "        \n",
    "        Summary:\"\"\"\n",
    "    )\n",
    "    \n",
    "    summarizer_chain = summarizer_prompt | llm | StrOutputParser()\n",
    "    summary = summarizer_chain.invoke({\"documents\": combined_docs})\n",
    "    \n",
    "    print(f\"Summarized documents to {len(summary)} chars\")\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"summarized_docs\": summary\n",
    "    }\n",
    "\n",
    "def generate_answer(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    \n",
    "    if state.get(\"needs_summary\") and state.get(\"summarized_docs\"):\n",
    "        context = state[\"summarized_docs\"]\n",
    "        print(\"Using summarized context for answer generation\")\n",
    "    else:\n",
    "        context = \"\\n\\n\".join(state[\"documents\"])\n",
    "        print(\"Using original documents for answer generation\")\n",
    "    \n",
    "    answer_prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Answer the question based on the following context:\n",
    "        \n",
    "        Context: {context}\n",
    "        \n",
    "        Question: {question}\n",
    "        \n",
    "        Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    answer_chain = answer_prompt | llm | StrOutputParser()\n",
    "    answer = answer_chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    return {\n",
    "        **state,\n",
    "        \"answer\": answer\n",
    "    }\n",
    "\n",
    "def should_summarize(state: GraphState) -> str:\n",
    "    if state.get(\"needs_summary\", False):\n",
    "        return \"summarize\"\n",
    "    return \"generate\"\n",
    "\n",
    "print(\"✓ Graph nodes defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "workflow.add_node(\"retrieve\", retrieve_documents)\n",
    "workflow.add_node(\"summarize\", summarize_documents)\n",
    "workflow.add_node(\"generate\", generate_answer)\n",
    "\n",
    "workflow.set_entry_point(\"retrieve\")\n",
    "\n",
    "workflow.add_conditional_edges(\n",
    "    \"retrieve\",\n",
    "    should_summarize,\n",
    "    {\n",
    "        \"summarize\": \"summarize\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "\n",
    "workflow.add_edge(\"summarize\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "graph = workflow.compile()\n",
    "\n",
    "print(\"✓ LangGraph workflow compiled\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LangGraph Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Test 1: Short query (should skip summarization)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"question\": \"Who directed The Matrix?\",\n",
    "    \"documents\": [],\n",
    "    \"summarized_docs\": \"\",\n",
    "    \"answer\": \"\",\n",
    "    \"needs_summary\": False\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Test 2: Complex query (should trigger summarization)\")\n",
    "print(\"=\" * 80 + \"\\n\")\n",
    "\n",
    "result = graph.invoke({\n",
    "    \"question\": \"Explain the plot of The Matrix, including the main characters and themes.\",\n",
    "    \"documents\": [],\n",
    "    \"summarized_docs\": \"\",\n",
    "    \"answer\": \"\",\n",
    "    \"needs_summary\": False\n",
    "})\n",
    "\n",
    "print(f\"\\nFinal Answer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Reflection - LangChain vs LangGraph\n",
    "\n",
    "### When to Use LangChain vs LangGraph\n",
    "\n",
    "**LangChain** is ideal for straightforward, linear workflows like RAG pipelines, chatbots, or simple question-answering systems. It excels at chaining components sequentially with minimal complexity. Use it when your workflow follows a predictable path and doesn't require complex branching logic.\n",
    "\n",
    "**LangGraph** shines in complex, stateful applications requiring conditional branching, loops, or multi-agent coordination. It's better suited for workflows where decisions determine the next step, such as autonomous agents, multi-step reasoning tasks, or systems needing human-in-the-loop interventions.\n",
    "\n",
    "**Trade-offs:**\n",
    "- LangChain offers simplicity and faster development but limited control over execution flow\n",
    "- LangGraph provides fine-grained control and handles complexity better but requires more setup and understanding of graph-based architectures\n",
    "- LangChain has better documentation and community support; LangGraph is newer with evolving patterns\n",
    "\n",
    "Choose based on complexity: simple chains → LangChain; complex decision trees → LangGraph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook successfully demonstrates:\n",
    "\n",
    "1. ✓ **LangChain with Conversation Memory**: Implemented a RAG system that maintains conversation context and correctly resolves references like 'the main character' and 'he' across multiple turns\n",
    "\n",
    "2. ✓ **LangGraph with Conditional Summarizer**: Built a workflow with intelligent document processing that:\n",
    "   - Retrieves relevant documents\n",
    "   - Conditionally summarizes if total length > 800 characters\n",
    "   - Generates final answer using appropriate context\n",
    "\n",
    "3. ✓ **Reflection**: Provided analysis of when to use each framework and their respective trade-offs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
